{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Presentation\n",
    "In this presentation we will be looking at the paper https://arxiv.org/pdf/1602.07261.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why ResNets?\n",
    "- The main driving factor is the belief that deeper networks are better than shallow ones (this is somewhat theoretically justified in some work, for example in \"Deep vs. Shallow Networks: an Approximation Theory Perspective\" by Mhaskar and Poggio\n",
    "- Experimentally, the previous winners of ImageNet had been improving by increasing network depth from 8 convolutional layers to around 20 layers. Was there a way to take this much further?\n",
    "\n",
    "<img src=\"https://i.imgur.com/StN3FtE.jpg\", width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://adeshpande3.github.io/assets/GoogLeNet5.png\",width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can try but...\n",
    "\n",
    "A problem arises when we try this in the manner of previous designs. Introducing more layers causes a 'degredation' problem, in where both the training and the test error increases when more layers were added. This is perhaps counterintuitive, as the added layers could be simply the identity mapping, and then we would expect performance as good as the shallower network.\n",
    "\n",
    "<img src=\"https://i.imgur.com/p7Ezh0o.png\",width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T22:52:24.957196Z",
     "start_time": "2017-11-27T22:52:24.949172Z"
    }
   },
   "source": [
    "## ResNets - a simple idea to avoid degredation\n",
    "This leads to the idea to add shortcuts, or residual connections. \n",
    "\n",
    "Imagine that the ideal mapping for a 2 layer network was $H(x)$, a standard ConvNet has to learn this mapping directly, a ResNet can now instead learn $H(x)-x$. While theoretically ConvNets should be able to handle both, practially, this may be a simpler function to learn.\n",
    "\n",
    "<img src=\"https://i.imgur.com/WxJZeH5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Restrictions\n",
    "The most obvious restriction imposed by this is that the number of pixels in an activation $x$ must be the same as in $F(x)$. The previous state of the art, VGGNet, has nearly this design.\n",
    "\n",
    "<img src=\"https://i.imgur.com/D8zcLta.jpg\">\n",
    "\n",
    "Groups of layers have the same size, but after 3 or 4 layers, the spatial size is reduced (adding location invariance), and the number of channels are increased (going from more repeatable simple shapes in early layers to less repeatable complex shapes in later layers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection Layers\n",
    "\n",
    "ResNets experimented with a few different ways to handle the resolution changes introduced by downsampling but increasing channel size. They experimentally found that simply having a normal convolutional layer (with a stride higher than 1) was a good way to handle this. I.e. there is no residual passing through this layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BottleNeck Layers and 1x1 Convolutions\n",
    "This is not so important, but is mentioned in the paper so it is worth looking at.\n",
    "\n",
    "The idea of $1\\times 1$ convolutions was used heavily in the inception model (the previous year's best model, alongside VGG). Having a $1\\times 1$ convolution is effectively the same as summing the channels of the activation with different gains. No spatial shift can be applied, but you can now have multiple different combinations of the previous layer's channels, and this is computationally very useful. Now we can reduce the channel dimension before doing larger spatial convolutions and then increase it again with more $1\\times 1$ convolutions.\n",
    "\n",
    "Microsoft do just that in their 'deeper' models with the BottleNeck layers.\n",
    "\n",
    "<img src=\"https://i.imgur.com/dzT3A0M.png\">\n",
    "\n",
    "<img src=\"https://i.imgur.com/BIwgy52.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BottleNeck Convolutions analysis\n",
    "Comparison of number of operations:\n",
    "\n",
    "Assume input has spatial dimensions $N \\times N$. The standard convolutional layer will have $C$ channels, and the bottleneck architecture will have $4C$.\n",
    "\n",
    "**Learned parameters**\n",
    "Standard convolution has $3\\times 3 \\times C \\times C + 3\\times 3 \\times C \\times C = 18C^2$ learned parameters\n",
    "\n",
    "Bottleneck convolution has $1 \\times 1 \\times 4C \\times C + 3\\times 3 \\times C \\times C + 1\\times 1\\times C\\times 4C = 17C^2$ learnable parameters\n",
    "\n",
    "**Computational Cost**\n",
    "Standard convolutional layer has spatial size $N \\times N$ pixels. Each spatial location for 1 filter uses $9C$ multiply-adds (madds), giving $9CN^2$ madds per filter, or $9C^2N^2$ madds for the first layer, and the same for the second.\n",
    "\n",
    "Bottleneck convolutional layer has the same spatial size. \n",
    "- The first filter uses $4C$ madds per spatial location, giving $4CN^2$ madds per filter, or $4C^2N^2$ madds in total (C filters). \n",
    "- The second layer uses $9C$ madds per spatial location, giving $9CN^2$ madds per filter, or $9C^2N^2$ madds in total (C filters).\n",
    "- The third layer uses $C$ madds per spatial location, giving $CN^2$ madds per filter, or $4C^2N^2$ madds in total (4C filters)\n",
    "\n",
    "This means the bottleneck convolution has similar parameter and computational cost ($17C^2$ vs $18C^2$ parameters and multiply-adds per spatial location), but now we've had 3 'layers' of convolution rather than 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Results\n",
    "<img src=\"https://i.imgur.com/w9o1T5q.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Analysis\n",
    "\n",
    "Firstly, Notice that resnet weights have less energy - learning the residual does break the task down.\n",
    "\n",
    "Secondly, the std doesn't go to 0 as the layer index increases, meaning that it is learning more than just the residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T00:50:47.049786Z",
     "start_time": "2017-11-28T00:50:47.025458Z"
    }
   },
   "source": [
    "<img src=\"https://i.imgur.com/aIcM2K1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Packages for loading images\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Numerical packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Package to restrict tensorflow to only use one gpu - see https://github.com/fbcotter/py3nvml.git\n",
    "try:\n",
    "    import py3nvml\n",
    "    py3nvml.grab_gpus(1)\n",
    "except:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "# Plotting packages\n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "214px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
